ğŸš€ Azure Serverless Sentiment Analysis Pipeline

A fully serverless, event-driven, and scalable sentiment analysis pipeline built with Azure Functions, Container Apps, Blob Storage, and Application Insights.

ğŸ“Œ Features

ğŸ”„ Event-driven CSV ingestion (Blob Trigger)

ğŸ“¦ Batch processing with configurable chunk sizes

âš¡ FastAPI ML inference served via Azure Container Apps

â˜ï¸ Fully serverless & scalable architecture

ğŸ“Š Centralized logging & monitoring (Application Insights)

ğŸ§¹ Clean, modular repository structure

ğŸ§­ Overview

A CSV file uploaded to Azure Blob Storage automatically triggers an Azure Function.
The function:

Reads and streams the file

Splits rows into batches (CHUNK_SIZE)

Calls the ML API (FastAPI in a container)

Writes output JSON chunks + a manifest file

Pushes logs/metrics to Application Insights

This architecture is ideal for large-scale batch ML predictions in a serverless environment.

ğŸ— Architecture Diagram (Mermaid)

flowchart LR

  U[User Upload<br>(Portal / az CLI / SDK)] --> A

  subgraph Storage[Azure Blob Storage]
    A[input/ (CSV files)]
    O[output/ (JSON chunks)]
  end

  A -->|Blob Trigger| F[Azure Function App<br>process_blob()]

  F -->|HTTP POST /predict<br>(CHUNK_SIZE batches)| C[Azure Container App<br>FastAPI ML API]

  C -->|JSON predictions| F

  F -->|Write output chunks| O
  F -->|Logs & Traces| AI[Application Insights]

  AI --> Dash[Dashboards Â· Logs Â· Alerts Â· Metrics]


ğŸ“ Repository Structure

.
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ inference.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ function/
â”‚   â”œâ”€â”€ function_app.py
â”‚   â”œâ”€â”€ host.json
â”‚   â”œâ”€â”€ local.settings.json
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ samples/
â”‚   â””â”€â”€ demo_small.csv
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture.png
â”‚
â””â”€â”€ .github/workflows/
    â””â”€â”€ build-push.yml

ğŸ”§ Setup Instructions (Full Azure Deployment)
1ï¸âƒ£ Build & Push the ML API Image
docker buildx build --platform linux/amd64 \
  -t $ACR_SERVER/mlapi:v1 \
  -f api/Dockerfile .

docker push $ACR_SERVER/mlapi:v1

2ï¸âƒ£ Deploy the Azure Container App (FastAPI Model)
az containerapp create \
  -g rg-mlserverless-dev \
  -n ca-mlapi-dev \
  --environment cae-mlserverless-dev \
  --image $ACR_SERVER/mlapi:v1 \
  --ingress external --target-port 8000 \
  --min-replicas 1 --cpu 1 --memory 2Gi \
  --registry-server $ACR_SERVER \
  --registry-username $ACR_USER \
  --registry-password $ACR_PASS

3ï¸âƒ£ Create Blob Storage Containers
az storage container create --account-name <storage> --name input
az storage container create --account-name <storage> --name output

4ï¸âƒ£ Deploy the Azure Function
cd function
func azure functionapp publish func-mlserverless-dev --build remote

5ï¸âƒ£ Configure Function App Settings
az functionapp config appsettings set \
  -g rg-mlserverless-dev \
  -n func-mlserverless-dev \
  --settings \
    API_URL="https://ca-mlapi-dev.<id>.francecentral.azurecontainerapps.io/predict" \
    CHUNK_SIZE=20 \
    TIMEOUT_SEC=180 \
    OUTPUT_CONTAINER=output

ğŸ§ª Testing the Pipeline
Small Test File
az storage blob upload \
  --account-name <storage> \
  --container-name input \
  --file samples/demo_small.csv \
  --name demo_small.csv


Expected output:

demo_small.csv.0000.json
demo_small.csv.manifest.json

Large CSV
az storage blob upload \
  --account-name <storage> \
  --container-name input \
  --file big.csv \
  --name input.csv


Pipeline behavior:

Streams CSV rows

Splits into batches

Sends to FastAPI ML model

Writes JSON chunks

Generates manifest file

Tune if needed:

CHUNK_SIZE=10
TIMEOUT_SEC=240

ğŸ“Š Monitoring (Application Insights)

Function logs

traces
| where timestamp > ago(2h)
| order by timestamp desc


API request logs

requests
| where timestamp > ago(2h)
| order by timestamp desc


Exceptions

exceptions
| where timestamp > ago(2h)

ğŸ›  Error Handling
Issue	Fix
â± Timeout	Increase TIMEOUT_SEC
ğŸ“¦ Batch too large	Reduce CHUNK_SIZE
ğŸ“„ Empty CSV	Writes minimal manifest
ğŸ”´ API down	Creates error chunk & continues


